---
title: "Topology meets the real world"
subtitle: "How geometry can help us analyse finite metric spaces"
project:
  output-dir: docs
author: "Guilherme Vituri F. Pinto*, Laura R. Gambera**"
institute: |
  *Head of Intelligence at Argus Solutions
  
  **Data Scientist at Argus Solutions
  
bibliography: references.bib

format: 
    revealjs:
        preview-links: true
        scrollable: true
        theme: dark
        footer: ""
        logo: logo.png
        chalkboard: true
        transition: slide
        transition-speed: fast
        self-contained: false
        incremental: true
        auto-animate: true
        fig-responsive: true
    pptx: default
---

```{r, include=FALSE}

library(vituripackage)
```

## Data in the wild

In many cases, real world datasets are **tables** and can be seen as a finite space $X \subset \mathbb{R}^n$ for some $n$, with some choice of metric to it.


```{r, background='white', fig.align='center'}
iris |> select(-Species) |> head(10) #|> reactable::reactable()
```

## Value from data

Means and percentage are cool, but we need more!

- Detect trendings and cycles in data;

- Correlations, causation and ways to avoid what is "bad" or maximize what is "good";

- Data science as a detective job;

- Example: why drivers sleep more on Thursday?

## Insights from topology

:::: {.columns}

::: {.column width="60%"}
![](images/dataset1.png)
:::

::: {.column width="40%"}

- discrete space;

- 1700 connected components;

- trivial $H_n$ for $n > 0$;

- call it a day and go to sleep.

:::

::::

##  {background-image="images/baby.jpg" background-size="contain" footer=false}

## Topology meets the real world

**Topological Data Analysis** builds bridges from:

- Topological spaces => finite datasets:

- Homology => persistent homology;

- Reeb graph => mapper graph;

- Connected components => single-linkage clustering.

# Problem 1: clustering

**Objective**: partition a dataset into "useful" groups of points. 

An analog for "connectivity" in the discrete case.

##  {background-image="images/cluster-comparison1.png" background-size="contain"}

##  {background-image="images/cluster-comparison2.png" background-size="contain"}

##  {background-image="images/cluster-comparison3.png" background-size="contain"}

##  {background-image="images/cluster-comparison.png" background-size="contain"}

<!-- ## Connectivity in topology -->

<!-- A space $X$ can be: -->

<!-- - Connected... -->

<!-- - Path-connected...... -->

<!-- - Arcwise-connected......... -->

<!-- - Simply connected............ -->

<!-- - $n$-connected............... -->

<!-- - Locally connected.................. -->

<!-- - Locally path-connected..................... -->

## Getting started {.smaller}

:::: {.columns}

::: {.column width="60%"}
![](images/dataset1.png)
:::

::: {.column width="40%" }

- $x_1$ and $x_2$ are measurements of something (facial features, temperature and pressure, etc.);

- small error are usual;

- the "true" value is in the center;

- the "true" value is closer to the other values than "false" values.
:::

::::

## The ToMATo clustering algorithm

In [@tomato], the authors define the *Topological Mode Analysis Tool*, or *ToMATo* algorithm. The intuitive idea is the following:

## The ToMATo clustering algorithm

::: {layout-ncol=2}
![Find poins in the most dense regions.](images/tomato-guess.png)

![Build the perimeter by a hill-climbing method.](images/tomato-guess2.png)
:::

## Step 1

Calculate a density estimator

![](images/tomato2.png){fig-align="center"}

## Plot the mountains

![](images/tomato3.png){fig-align="center"}

## Step 2

Create a proximity graph

![](images/tomato4.png){fig-align="center"}

## Step 3

Chop the mountains! 

Reindex $x_1, ..., x_n \in X$ from highest to lowest.

Start with $x = x_1$.

> If no neighbor of $x$ is higher than $x$, then $x$ form a cluster $c_1$

otherwise

> $x$ will be assigned to the cluster of its highest neighbor.

Keep going until all $x \in X$ is assigned to a cluster.

## Details
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg1){width=100%}
:::
::: {.column width="45%"}
Arrange index set
:::
::::

## x = x_1
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg2){width=100%}
:::
::: {.column width="45%"}
no neighbor of $x_1$ is higher than $x_1$, then $x_1$ form a cluster $c_1$
:::
::::

## x = x_2
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg3){width=100%}
:::
::: {.column width="45%"}
no neighbor of $x_2$ is higher than $x_2$, then $x_2$ form a cluster $c_2$
:::
::::

## x = x_3
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg4){width=100%}
:::
::: {.column width="45%"}
$x_3$ will be assigned to the cluster of its highest neighbor, $c_1$
:::
::::

## x = x_4
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg5){width=100%}
:::
::: {.column width="45%"}
$x_4$ will be assigned to the cluster of its highest neighbor, $c_2$
:::
::::

## :(

![](images/tomato5.png){fig-align="center"}

## Throwing away the false peaks

> If a neighbor of $x$ is a peak just a bit higher than $x$, then merge its cluster to the one of $x$.

## Measure
:::: {.columns}
::: {.column width="45%"}
![](images/tomato-adjust1.png){width=100%}
:::
::: {.column width="55%"}
$| \text{dens}(x_2) - \text{dens}(x_3) | < \tau;$
:::
::::

## Merge
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-adjust2.png){width=100%}
:::
::: {.column width="45%"}
$x_2$ and $x_4$ are assigned to $c_1$.
:::
::::

## Final result with $\tau$ = 0.02

![](images/tomato6.png){fig-align="center"}

## Result with $\tau$ = 0.04

![](images/tomato7.png){fig-align="center"}

## ToMATo: two spirals with noise 

```{r, warning=FALSE, fig.align='center'}
library(magick)
imgs = 
  paste0('images/spiral', 1:4, '.png') %>% 
  purrr::map(image_read) %>% 
  purrr::map(\(x) image_scale(x, "500x400")
  )

image_append(
  c(
    image_append(c(imgs[[1]], imgs[[2]]))
    ,image_append(c(imgs[[3]], imgs[[4]]))
  )
  ,stack = TRUE
)
```


## ToMATo: components with noise {.r-stretch}

```{r, fig.align='center'}
imgs = 
  paste0('images/toy', 1:4, '.png') %>% 
  purrr::map(image_read) %>% 
  purrr::map(\(x) image_scale(x, "500x400"))

image_append(
  c(
    image_append(c(imgs[[1]], imgs[[2]]))
    ,image_append(c(imgs[[3]], imgs[[4]]))
  )
  ,stack = TRUE
)

```


# Problem 2: signatures

**Objective**: given a complicated object $X$, create a simpler object $S(X)$ which can give information about $X$.

. . .

Example: $X$ and $Y$ are hard to compare, but 

$$
S(X) \neq S(Y) \Rightarrow X \neq Y.
$$ 

## Connected components

Clustering algorithms depend on several parameters.

. . .

Is there a more natural way to count connected components on finite sets?

## From finite to infinite

- Center a ball of radius $\epsilon$ in each point of $X$ and make $\epsilon$ vary. 

- Call $X_\epsilon$ the union of these balls, and $K_\epsilon$ the nerve of $X_\epsilon$.

- Count the connected components of $X_\epsilon$ or $K_\epsilon$ for each $\epsilon$.

![](images/filtration.png)

## Vietoris-Rips complex

In the previous construction, $K_\epsilon$ is called the *Vietoris-Rips* complex of $X$ at parameter $\epsilon$.

![](images/vietoris-rips.png){fig-align="center"}

## Dendrograms

A dendrogram represents the birth and death of each connected component. It is *agglomerative*.

```{r}
#| layout-ncol: 2
#| fig-width: 5
#| fig-height: 4

n = 25
X = rbind(
  runif(200) |> matrix(ncol = 2)
  ,runif(200, min = 1.2, max = 2.2) |> matrix(ncol = 2)
)

hc = hclust(dist(X, method = 'euclidean'), method = 'single')

plot(X)
plot(hc, labels = FALSE, hang = -1)
```

## Enters topology

- Connected components are measured by the $H_0$ functor.

- Dendrograms keep track of $H_0$ generators.

- Can we use $H_n$ instead of $H_0$???????????????

## Persistent homology

Given a family of simplicial complexes

$$
K_0 \hookrightarrow K_1 \hookrightarrow \cdots \hookrightarrow K_n
$$

. . .

apply homology $H_p$ over a field $\mathbb{K}$

$$
H_p(K_0) \rightarrow H_p(K_1) \rightarrow \cdots \rightarrow H_p(K_n)
$$

to obtain a sequence of vector spaces and linear maps, called a *persistence module*.

## Barcodes

A persistence module $\mathbb{V}$ can be decomposed as a sum of *interval modules*, which gives us a multiset of intervals called *barcode*

$$
\text{dgm}(\mathbb{V}) = \{[b, d); \; (b, d) \in A\}.
$$

Each interval $[b, d) \in \text{dgm}(\mathbb{V})$ can be seem as a generator of $H_p$ (ie. a $p$-dimensional hole) that was "born" at $K_b$ and "died" at $K_d$.

## Birth and death of "holes"

![](images/barcode.png){fig-align="center"}

## Stability

Small changes in $X$ will change just a little of the barcodes. 

Theorem (!! referencia): given $X$ and $Y$ finite metric spaces,

$$
d_b(\text{dgm}(X), \text{dgm}(Y)) \leq d_{GH}(X, Y).
$$

. . .

Thus:

$$
d_b(\text{dgm}(X), \text{dgm}(Y)) \; \text{is big} \Rightarrow \\ 
\text{$X$ and $Y$ are very different}.
$$

## The complex factory

There are several way to build sequences of complex:

- Vietoris-Rips, ÄŒech, Alpha complex;

- Sub-level or super-level filtration.

## Sublevel filtration

Given $f: X \to \mathbb{R}$, consider for any $\epsilon \in \mathbb{R}$ the inverse image
$$
X_\epsilon = f^{-1}((-\infty, \epsilon])
$$
and proceed to calculate its persistent homology.

![Source: [@ripserer]](images/sublevel2.png){fig-align="center"}

## Using!

!!! We can use the distance between barcodes as an approximation of the distance between spaces.

## The magic of vectors

Machine learning algorithms need vectors (or matrices) of a fixed size as input.

So, in order to use barcodes in machine learning, we need to vectorize them!

## From barcodes to matrices {.smaller}

One successful construction [@adams2017persistence] is the *persistence image*. The idea is the following:

- Plot the persistence diagram;

- Put gaussians around each point;

- Pixelate them.

![](images/persistence-images.png){fig-align="center"}

## Example

Using persistence homology to classify hand-written digits (see [@tda_with_julia] for details).

![](images/digits.png){fig-align="center"}

## Steps

::: {layout="[[1,1], [1]]"}
![1) Calculate a density estimator](images/digits2.svg)

![2) Do superlevel filtration](images/digits3.png)

![3) Do sublevel filtration](images/digits3.png)


:::


# Problem 3: dimensionality reduction

**Objective**: See what can't be seen!

Given a high-dimensional dataset $X$, how can we visualize it while keeping some of its structure?

## Reeb graph

:::: {.columns}

::: {.column width="55%"}

![](images/reeb.png){width=100%}
:::

::: {.column width="45%"}

Given a topological space $X$ and a map $f: X \to \mathbb{R}$, define $\sim$ on $X$ such that

$$
p \sim q \Leftrightarrow
\text{$p, q \in f^{-1}(c)$} \\ \text{for some $c \in \mathbb{R}$.}
$$

The Reeb Graph of $(X, f)$ is the quotient $X / \sim$
:::
::::

## From infinite to finite

To adapt the Reeb graph to the finite case, we need to change:

- $X$ topological space => $(X, d)$ finite metric space;
- $f: X \to \mathbb{R}$ continuous => $f: X \to \mathbb{R}$ any function;
- inverse images of *points* of $\mathbb{R}$ => inverse images of *intervals* of $\mathbb{R}$;
- connected components of $X$ => clusters of $X$.

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper1.png){width=90%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper2.png){width=70%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper3.png){width=70%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;

3) Apply a clustering on the inverse image of each interval;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper4.png){width=80%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;

3) Apply a clustering on the inverse image of each interval;

4) Apply the nerve construction on the sets of clusters.
:::
::::

##  {background-image="images/mapper.png" background-size="contain"}

## A humble example
::: {layout-ncol=2}
![$X$ = torus, $f$ = projection, colored by $f$.](images/mapper-torus.png)

![The mapper graph.](images/mapper-torus2.png)
:::


# Closing remarks

- Study topology;

- Study data analysis;

- Learn to code! Start with Julia, Python or R.

- Most important: **combine the three above**.

# ...

# ...................

# Obrigado!!

## References

<!-- http://education.knoweng.org/clustereng/algorithms.php#comparison -->
