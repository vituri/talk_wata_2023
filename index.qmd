---
title: "Topology meets the real world"
subtitle: "How geometry can help us analyse finite metric spaces"
project:
  output-dir: docs
author: "Guilherme Vituri F. Pinto*, Laura R. Gambera**"
institute: |
  *Head of Intelligence at Argus Solutions
  
  **Data Scientist at Argus Solutions
  
bibliography: references.bib

format: 
    revealjs:
        preview-links: true
        scrollable: true
        theme: dark
        footer: "Workshop on Algebraic Topology and Applications, 2023"
        logo: logo.png
        chalkboard: true
        transition: slide
        transition-speed: fast
        self-contained: false
        incremental: true
        auto-animate: true
        fig-responsive: true
    pptx: default
---

```{r, include=FALSE}

library(vituripackage)
```

## Data in the wild

In many cases, real world datasets are **tables** and can be seen as a finite space $X \subset \mathbb{R}^n$ for some $n$, with some choice of metric to it.


```{r, background='white', fig.align='center'}
iris |> select(-Species) |> head(10) #|> reactable::reactable()
```

## Value from data

Means and percentage are cool, but we need more!

- Detect trendings and cycles in data;

- Correlations, causation and ways to avoid what is "bad" or maximize what is "good";

- Data science as a detective job;

- Example: why drivers sleep more on Thursday?

## Insights from topology

:::: {.columns}

::: {.column width="60%"}
![](images/dataset1.png)
:::

::: {.column width="40%"}

- discrete space;

- 1700 connected components;

- trivial $H_n$ for $n > 0$;

- call it a day and go to sleep.

:::

::::

##  {background-image="images/baby.jpg" background-size="contain" footer=false}

## Topology meets the real world

**Topological Data Analysis** builds bridges from:

- Topological spaces => finite datasets:

- Homology => persistent homology;

- Reeb graph => mapper graph;

- Connected components => single-linkage clustering.

# Problem 1: clustering

**Objective**: partition a dataset into "useful" groups of points. 

An analog for "connectivity" in the discrete case.

##  {background-image="images/cluster-comparison1.png" background-size="contain"}

##  {background-image="images/cluster-comparison2.png" background-size="contain"}

##  {background-image="images/cluster-comparison3.png" background-size="contain"}

##  {background-image="images/cluster-comparison.png" background-size="contain"}

<!-- ## Connectivity in topology -->

<!-- A space $X$ can be: -->

<!-- - Connected... -->

<!-- - Path-connected...... -->

<!-- - Arcwise-connected......... -->

<!-- - Simply connected............ -->

<!-- - $n$-connected............... -->

<!-- - Locally connected.................. -->

<!-- - Locally path-connected..................... -->

## Getting started {.smaller}

:::: {.columns}

::: {.column width="60%"}
![](images/dataset1.png)
:::

::: {.column width="40%" }

- $x_1$ and $x_2$ are measurements of something (facial features, temperature and pressure, etc.);

- small error are usual;

- the "true" value is in the center;

- the "true" value is closer to the other values than "false" values.
:::

::::

## ToMATo!

In [@tomato], the authors define the *Topological Mode Analysis Tool*, or *ToMATo* algorithm. The intuitive idea is the following:

## ToMATo!

::: {layout-ncol=2}
![Find poins in the most dense regions.](images/tomato-guess.png)

![Build the perimeter by a hill-climbing method.](images/tomato-guess2.png)
:::

## Step 1

Calculate a density estimator

![](images/tomato2.png){fig-align="center"}

## Plot the mountains

![](images/tomato3.png){fig-align="center"}

## Step 2

Create a proximity graph

![](images/tomato4.png){fig-align="center"}

## Step 3

Chop the mountains! 

Reindex $x_1, ..., x_n \in X$ from highest to lowest.

. . .

Start with $x = x_1$.

. . .

> If no neighbor of $x$ is higher than $x$, then $x$ form a cluster $c_1$

. . .

otherwise

> $x$ will be assigned to the cluster of its highest neighbor.

. . .

Keep going until all $x \in X$ is assigned to a cluster.

## Details
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg1){width=100%}
:::
::: {.column width="45%"}
Arrange index set
:::
::::

## x = x_1
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg2){width=100%}
:::
::: {.column width="45%"}
no neighbor of $x_1$ is higher than $x_1$, then $x_1$ form a cluster $c_1$
:::
::::

## x = x_2
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg3){width=100%}
:::
::: {.column width="45%"}
no neighbor of $x_2$ is higher than $x_2$, then $x_2$ form a cluster $c_2$
:::
::::

## x = x_3
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg4){width=100%}
:::
::: {.column width="45%"}
$x_3$ will be assigned to the cluster of its highest neighbor, $c_1$
:::
::::

## x = x_4
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-alg5){width=100%}
:::
::: {.column width="45%"}
$x_4$ will be assigned to the cluster of its highest neighbor, $c_2$
:::
::::

## :(

![](images/tomato5.png){fig-align="center"}

## Terraforming

We had a lot of peaks that were *too small* to be considered as a true peak.

. . .

Add this step to the algorithm:

> If a neighbor of $x$ is a peak just a bit higher than $x$, then merge its cluster to the one of $x$.

## Measure
:::: {.columns}
::: {.column width="45%"}
![](images/tomato-adjust1.png){width=100%}
:::
::: {.column width="55%"}
$| \text{dens}(x_2) - \text{dens}(x_3) | < \tau;$
:::
::::

## Merge
:::: {.columns}
::: {.column width="55%"}
![](images/tomato-adjust2.png){width=100%}
:::
::: {.column width="45%"}
$x_2$ and $x_4$ are assigned to $c_1$.
:::
::::

## Final result with $\tau$ = 0.02

![](images/tomato6.png){fig-align="center"}

## Result with $\tau$ = 0.04

![](images/tomato7.png){fig-align="center"}

## ToMATo: two spirals with noise 

```{r, warning=FALSE, fig.align='center'}
library(magick)
imgs = 
  paste0('images/spiral', 1:4, '.png') %>% 
  purrr::map(image_read) %>% 
  purrr::map(\(x) image_scale(x, "500x400")
  )

image_append(
  c(
    image_append(c(imgs[[1]], imgs[[2]]))
    ,image_append(c(imgs[[3]], imgs[[4]]))
  )
  ,stack = TRUE
)
```


## ToMATo: components with noise {.r-stretch}

```{r, fig.align='center'}
imgs = 
  paste0('images/toy', 1:4, '.png') %>% 
  purrr::map(image_read) %>% 
  purrr::map(\(x) image_scale(x, "500x400"))

image_append(
  c(
    image_append(c(imgs[[1]], imgs[[2]]))
    ,image_append(c(imgs[[3]], imgs[[4]]))
  )
  ,stack = TRUE
)

```


# Problem 2: signatures

**Objective**: given a complicated object $X$, create a simpler object $S(X)$ which can give information about $X$.

. . .

Example: $X$ and $Y$ are hard to compare, but 

$$
S(X) \neq S(Y) \Rightarrow X \neq Y.
$$ 

## Connected components

Clustering algorithms depend on several parameters.

. . .

Is there a more natural way to count connected components on finite sets?

## From finite to infinite

- Center a ball of radius $\epsilon$ in each point of $X$ and make $\epsilon$ vary. 
- $X_\epsilon :=$ the union of these balls; $K_\epsilon :=$ the nerve of $X_\epsilon$.

- Count the connected components of $X_\epsilon$ or $K_\epsilon$ for each $\epsilon$.

![](images/filtration.png)

## Vietoris-Rips complex

In the previous construction, $K_\epsilon$ is called the *Vietoris-Rips* complex of $X$ at parameter $\epsilon$.

![](images/vietoris-rips.png){fig-align="center"}

## Dendrograms

A dendrogram represents the birth and death of each connected component. It is *agglomerative*.

```{r}
#| layout-ncol: 2
#| fig-width: 5
#| fig-height: 4

n = 25
X = rbind(
  runif(200) |> matrix(ncol = 2)
  ,runif(200, min = 1.2, max = 2.2) |> matrix(ncol = 2)
)

hc = hclust(dist(X, method = 'euclidean'), method = 'single')

plot(X)
plot(hc, labels = FALSE, hang = -1)
```

## Enters topology

- Connected components are measured by the $H_0$ functor.

- Dendrograms keep track of $H_0$ generators.

- Can we use $H_n$ instead of $H_0$???????????????

## Persistent homology

Given a family of simplicial complexes

$$
K_0 \hookrightarrow K_1 \hookrightarrow \cdots \hookrightarrow K_n
$$

. . .

apply homology $H_p$ over a field $\mathbb{K}$

$$
H_p(K_0) \rightarrow H_p(K_1) \rightarrow \cdots \rightarrow H_p(K_n)
$$

to obtain a sequence of vector spaces and linear maps, called a *persistence module*.

## Barcodes

A persistence module $\mathbb{V}$ can be decomposed as a sum of *interval modules*, which gives us a multiset of intervals called *barcode*

$$
\text{dgm}(\mathbb{V}) = \{[b, d); \; (b, d) \in A\}.
$$

. . . 

Each interval $[b, d) \in \text{dgm}(\mathbb{V})$ can be seem as a generator of $H_p$ (ie. a $p$-dimensional hole) that was "born" at $K_b$ and "died" at $K_d$.

## Birth and death of "holes"

![](images/barcode.png){fig-align="center"}

## Stability

Small changes in $X$ will change just a little of the barcodes. 

Theorem: given $X$ and $Y$ finite metric spaces,

$$
d_b(\text{dgm}(X), \text{dgm}(Y)) \leq d_{GH}(X, Y).
$$

. . .

Thus:

$$
d_b(\text{dgm}(X), \text{dgm}(Y)) \; \text{is big} \Rightarrow \\ 
\text{$X$ and $Y$ are very different}.
$$

## The complex factory

There are several way to build sequences of complex:

- Vietoris-Rips complex;

- ÄŒech complex;

- Alpha complex;

- Sub-level or super-level filtration.

## Sublevel filtration

Given $f: X \to \mathbb{R}$, consider for any $\epsilon \in \mathbb{R}$ the inverse image
$$
X_\epsilon = f^{-1}((-\infty, \epsilon])
$$
and proceed to calculate its persistent homology.

![Source: [@ripserer]](images/sublevel2.png){fig-align="center"}


## The magic of vectors

Machine learning algorithms need vectors (or matrices) of a fixed size as input.

. . .

So, in order to use barcodes in machine learning, we need to vectorize them!

## From barcodes to matrices {.smaller}

One successful construction [@adams2017persistence] is the *persistence image*. The idea is the following:

- Plot the persistence diagram;

- Plot gaussians around each point;

- Pixelate them.

![](images/persistence-images.png){fig-align="center"}

## Example

Using persistence homology to classify hand-written digits (see [@tda_with_julia] for details).

![](images/digits.png){fig-align="center"}

<!-- ## Step 0 -->

<!-- - All digits are connected, thus $H_0$ will be trivial. -->

<!-- - $H_1$ will put many digits together. -->

<!-- - Better not use Vietoris-Rips complex! -->

## Step 1

Calculate an excentricity estimator

![](images/digits2.svg){fig-align="center"}

## Step 2

Do superlevel filtration

![](images/digits3.png){fig-align="center"}

## Step 3

Calculate the barcode and persistence images of each dimension

![](images/digits4.png){fig-align="center"}

## Step 4

What do we do with all these matrices?????????????

. . .

Enters machine learning!

## Step 4 (magic)

Create a neural network that will take the persistence images as input.

. . .

Let it learn by itself!

## Neural networks

- Local transformation of a space into classes;

- Tries to maximize the separation of these classes.

!["Crumple together two sheets of paper into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem." [@nn]](images/nn.png)


## Step 5

How good was our neural network?

. . .

After learning with 10.000 persistence images, we got an accuracy of

<h1> <center>
71%
</h1> </center>

. . .

which is pretty bad...

. . .

<h1> <center>
:(
</h1> </center>

## The perils of isometry {.nonincremental}

Isometric spaces will have the same barcodes using the density function!

- 6 and 9 are isometric;

- 2 and 5 are very similar.

## Improving

To get a better result, give more data to the neural network!

- Create the sub-level filtration of the digits, using the projection on the x and y axis;

- Calculate the barcodes and persistence images;

- Concatenate all these persistence images in a big vector;

- Feed the neural network.

## Results

Now our accuracy rose to 

<h2> <center>
95.1%
</h2> </center>

which seems pretty good for such naive approach!

![The confusion matrix of our neural network classifier. Source [@tda_with_julia]](images/digits5.png){fig-align="center"}

## The limits of topology

Plotting a sample of digits that our algorithm got wrong, we can see many ugly digits.

![](images/digits-errors.png){fig-align="center"}


# Problem 3: dimensionality reduction

**Objective**: See what can't be seen!

Given a high-dimensional dataset $X$, how can we visualize it while keeping some of its structure?

## Reeb graph

:::: {.columns}

::: {.column width="55%"}

![](images/reeb.png){width=100%}
:::

::: {.column width="45%"}

Given a topological space $X$ and a map $f: X \to \mathbb{R}$, define $\sim$ on $X$ such that

$$
p \sim q \Leftrightarrow
\text{$p, q \in f^{-1}(c)$} \\ \text{for some $c \in \mathbb{R}$.}
$$

The Reeb Graph of $(X, f)$ is the quotient $X / \sim$
:::
::::

## From infinite to finite

To adapt the Reeb graph to the finite case, we need to change:

- $X$ topological space => $(X, d)$ finite metric space;
- $f: X \to \mathbb{R}$ continuous => $f: X \to \mathbb{R}$ any function;
- inverse images of *points* of $\mathbb{R}$ => inverse images of *intervals* of $\mathbb{R}$;
- connected components of $X$ => clusters of $X$.

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper1.png){width=90%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper2.png){width=70%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper3.png){width=70%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;

3) Apply a clustering on the inverse image of each interval;
:::
::::

## The Mapper graph
:::: {.columns}
::: {.column width="50%"}
![](images/mapper4.png){width=80%}
:::
::: {.column width="50%" .nonincremental}
1) Define $f: X \to \mathbb{R}$;

2) Choose a interval covering of $f(X)$;

3) Apply a clustering on the inverse image of each interval;

4) Apply the nerve construction on the sets of clusters.
:::
::::

##  {background-image="images/mapper.png" background-size="contain"}

## A humble example
::: {layout-ncol=2}
![$X$ = torus, $f$ = projection, colored by $f$.](images/mapper-torus.png)

![The mapper graph.](images/mapper-torus2.png)
:::

## Application

Identifying type-2 diabetes.

![Source: [@diabetes-mapper]](images/mapper-app1.png){fig-align="center"}

## Application two

Identifying of subgroup of breast cancer with excellent survival.

![Source: [@mapper-cancer]](images/mapper-app2.jpeg){fig-align="center"}

## One more application

The mapper identified more distinct styles of playing basketball than the traditional 5.

![Two applications of the mapper algorithm using different coverings. Source: [@mapper-basket]](images/mapper-app3.png){fig-align="center"}

## Distances on mapper

We can also use the mapper as a reduction of a space $X$.

![Source: [@mapper-r]](images/mapper-gh.png){fig-align="center"}

## The general mapper

Looking from above, the mapper algorithm consists of two steps:

1) (covering step) Given a metric space $(X, d)$, create a covering $C$ of $X$;
1) (nerve step) Using $C$ as vertex set, create a graph.

. . .

In the classical mapper context, $C$ is generated using the clustering of pre-images of a function $f: X \to \mathbb{R}$. 

## Ball mapper

The ball mapper algorithm [@dlotko2019ball] in short:

1) Cover $X$ with balls of radius $\epsilon$;
1) Calculate the nerve.

```{r, warning=FALSE, fig.align='center'}
library(magick)
imgs = 
  paste0('images/bm', 1:2, '.png') %>% 
  purrr::map(image_read) %>% 
  purrr::map(\(x) image_scale(x, "500x400")
  )

image_append(c(imgs[[1]], imgs[[2]]))
```

## Ball mapper of torus, $\epsilon = 0.5$

![](images/bm3.png){fig-align="center"}

## Ball mapper of torus, $\epsilon = 0.8$

![](images/bm4.png){fig-align="center"}

## Ball mapper of torus, $\epsilon = 1.3$

![](images/bm5.png){fig-align="center"}

# Closing remarks

## Topology is not enough

TDA should be **one more tool** in your data analysis arsenal, **not the only one**.

. . .

Study TDA in the context of other algorithms which are already popular.

## Advice from an old man

- Study topology;

- Study data analysis;

- Learn to code! Julia, Python or R to start.

- Most important: **combine the three above**. 

. . . 

<h2><center>
Apply TDA!
</h2></center>

# ...

# ...................

# Obrigado!!

## References
